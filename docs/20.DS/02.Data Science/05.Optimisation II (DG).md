---
title: Optimisation II (DG)
date: 2022-09-20 15:13:26
permalink: /pages/109496/
categories:
  - DS
  - Data Science
tags:
  - 
author: 
  name: Jennifer
  link: https://github.com/Li-Jennifer
---
```
-   what first-order optimisation is
-   how gradient descent works
-   what automatic differentiation is, and why it is important for optimisation
-   what the Jacobian (matrix), gradient (vector) and Hessian (matrix) are, and how they characterise the local behaviour of the objective function
-   what continuity is, and how Lipschitz continuity relates to gradient descent
-   how stochastic relaxation can be used to create smooth continuous objective functions from discontinuous ones
-   what stochastic gradient descent is and why it is useful
-   what momentum is, and how it can improve gradient descent
-   what second-order methods are and their limitations
1. 什么是一阶优化
2. 梯度下降是如何工作的
3. 什么是自动差异化，为什么它对优化很重要
4. 雅可比矩阵(矩阵)、梯度(向量)和黑森矩阵(矩阵)是什么，以及它们如何描述目标函数的局部行为
5. 什么是连续性，以及Lipschitz连续性与梯度下降的关系
6. 如何利用随机弛豫从不连续目标函数生成平滑连续目标函数
7. 什么是随机梯度下降，为什么它是有用的
8. 什么是动量，它如何改善梯度下降
9. 什么是二阶方法以及它们的局限性
```

---
