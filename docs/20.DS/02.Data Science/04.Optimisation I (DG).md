---
title: Optimisation I (DG)
date: 2022-09-20 15:13:14
permalink: /pages/c2175c/
categories:
  - DS
  - Data Science
tags:
  - 
author: 
  name: Jennifer
  link: https://github.com/Li-Jennifer
---
1. 在优化的背景下，好的目标函数、约束函数和参数向量
2. 离散优化和连续优化的区别
3. 凸优化和非凸优化以及如何识别它们
4. 什么是约束优化以及软约束和硬约束的区别
5. 目标函数的主要性质，包括凸性和连续性
6. 优化的基本用法以及如何提出目标函数
7. 什么是线性最小二乘
8. 迭代优化是如何工作的
9. 启发式优化的原理
10. 具有元启发式的随机搜索特性:局部性、记忆性、温度和种群
---
## 1. Introduce to Optimization(优化)
1. Optimization: the process of adjusting things to make them better.
2. Parameters (参数):   **θ**
	1. the things we can adjust 
	2. parameters exist in a **parameter space**
3. objective function (目标函数) ： **L(θ)**
	1. Function that maps the parameters onto a _single numerical measure_ of how good the configuration is 
	2. the _loss function_, the _cost function_, _fitness function_, _utility function_, _energy surface_
	3. ("objective") measure of "goodness".

$$
\theta^* = \argmin_{\theta\in\Theta} L(\theta)
$$
4. Constraints: the feasible set / region
5. Minimizing differences:
	- distance between an output and a reference  
$$ 
\ L(\theta) = \|y^\prime - y\| = \|f(\vec{x};\theta) - y\| 
$$
## 2. Discrete vs continuous
focus on optimization $\real^n$. That is $$\theta \in \real^n = [\theta_1, \theta_2, \dots, \theta_n],$$ and the optimization problem is one of:

$$\theta^* = \argmin_{\theta \in \real^n} L(\theta), \text{subject to constraints}$$