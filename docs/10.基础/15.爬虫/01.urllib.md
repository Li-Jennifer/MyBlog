---
title: urllib
date: 2022-09-06 15:41:06
permalink: /pages/ba8af3/
categories:
  - 基础
  - 爬虫
tags:
  - 
author: 
  name: Jennifer
  link: https://github.com/Li-Jennifer
---


## 1. urllib 使用 
```python
# 使用urllib来获取百度首页的源码
import urllib.request  

# (1)定义一个url 就是你要访问的地址
url = 'http://www.baidu.com'

# (2)模拟浏览器向服务器发送请求 response响应
response = urllib.request.urlopen(url)

# （3）获取响应中的页面的源码 content 内容的意思
# read方法 返回的是字节形式的二进制数据
content = response.read().decode('utf-8')
# （4）打印数据
print(content)
```
### 1个类型 6个方法
```python
response类型是httpResponse
type(response) # <class 'http.client.HTTPResponse'>


read() 字节形式读取二进制 扩展：rede(5)返回前几个字节

readline() 读取一行

readlines() 一行一行读取 直至结束

getcode() 获取状态码

geturl() 获取url

getheaders() 获取headers
```
### 下载  urlretrieve()
```python
1.网页
url_page = 'http://www.baidu.com'
# url代表的是下载的路径 filename文件的名字
# 在python中 可以变量的名字 也可以直接写值
urllib.request.urlretrieve(url_page,'baidu.html')

2.下载图片
url_img = 'https://img1.baidu.com/it/u=3004965690,4089234593&fm=26&fmt=auto&gp=0.jpg'
urllib.request.urlretrieve(url= url_img,filename='lisa.jpg')

3. 下载视频

url_video = 'https://vd3.bdstatic.com/mda-mhkku4ndaka5etk3/1080p/cae_h264/1629557146541497769/mda-mhkku4ndaka5etk3.mp4?v_from_s=hkapp-haokan-tucheng&auth_key=1629687514-0-0-7ed57ed7d1168bb1f06d18a4ea214300&bcevod_channel=searchbox_feed&pd=1&pt=3&abtest='
urllib.request.urlretrieve(url_video,'hxekyyds.mp4')
```
## 2.请求对象定制
### 组成
```python 
url = 'https://www.baidu.com'
https://www.baidu.com/s?wd=周杰伦

http/https www.baidu.com 80/443 s wd = 周杰伦 
 协议 主机 端口号 路径 参数 锚点

headers = {
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}
```
#### 协议端口
| 协议   | 端口 |
| ------ | ---- |
| http   | 80   |
| https  | 443  |
| mysql  | 3306 |
| oracle | 1521 |
| redis  | 6379 |
| mongodb       |   27017   |
### 编码解码
字节->字符串：解码 decode
字符串->字节：编码 encode
#### get：urllib.parse.quote() 单个参数
```python
import urllib.request
import urllib.parse
url = 'https://www.baidu.com/s?wd='

# 请求对象的定制为了解决反爬的第一种手段
headers = {
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:103.0) Gecko/20100101 Firefox/103.0'
}

# 将周杰伦三个字变成unicode编码的格式
# 我们需要依赖于urllib.parse
name = urllib.parse.quote('周杰伦')
url = url + name

# 请求对象的定制
request = urllib.request.Request(url=url,headers=headers)

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)

# 获取响应的内容
content = response.read().decode('utf-8')

# 打印数据
print(content)

```
#### get: urllib.parse.urlencode() 多个参数
```python
import urllib.request
import urllib.parse
base_url = 'https://www.baidu.com/s?'

data = {
'wd':'周杰伦',
'sex':'男',
'location':'中国台湾省'
}

new_data = urllib.parse.urlencode(data)

# 请求资源路径
url = base_url + new_data

headers = {
'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:103.0) Gecko/20100101 Firefox/103.0'
}
  
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
print(response.read().decode('utf‐8'))
```
#### post
```python
url = 'https://fanyi.baidu.com/sug'
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:103.0) Gecko/20100101 Firefox/103.0'
}
  
data = {
    'kw':'spider'
}

# post请求的参数 必须要进行编码
data = urllib.parse.urlencode(data).encode('utf-8')
request = urllib.request.Request(url=url,data=data,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')

# 字符串--》json对象
import json
obj = json.loads(content)
print(obj)

# post请求方式的参数 必须编码 data = urllib.parse.urlencode(data)
# 编码之后 必须调用encode方法 data = urllib.parse.urlencode(data).encode('utf-8')
# 参数是放在请求对象定制的方法中 request = urllib.request.Request(url=url,data=data,headers=headers)
```
####  ajax post
```python
import urllib.request
import urllib.parse

def create_request(page):
    base_url = 'https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&'
    data = {
    'start':(page - 1) * 20,
    'limit':20
    }

    data = urllib.parse.urlencode(data)
    url = base_url + data
    headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'
    }
    request = urllib.request.Request(url = url,headers = headers)
    return request

def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
        return content

def down_load(page,content):
    with open('douban_' + str(page) + '.json','w',encoding='utf-8')as fp:
        fp.write(content)


# 程序的入口
if __name__ == '__main__':
    start_page = int(input('请输入起始的页码'))
    end_page = int(input('请输入结束的页面'))
    for page in range(start_page,end_page+1):

    # 每一页都有自己的请求对象的定制
    request = create_request(page)
    # 获取响应的数
    content = get_content(request)
    # 下载
    down_load(page,content)
```

## 3.异常
### URLError / HTTPError
1. HTTPError类是URLError类的子类
2. 导入的包urllib.error.HTTPError urllib.error.URLError
3. http错误：http错误是针对浏览器无法连接到服务器而增加出来的错误提示。引导并告诉浏览者该页是哪里出了问题。
4. 通过urllib发送请求的时候，有可能会发送失败，这个时候如果想让你的代码更加的健壮，可以通过try except进行捕获异常，异常有两类，URLError\HTTPError
```python
import urllib.request
import urllib.error

# url = 'https://blog.csdn.net/sulixu/article/details/1198189491'
url = 'http://www.doudan1111.com'

headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:103.0) Gecko/20100101 Firefox/103.0'
}

try:
    request = urllib.request.Request(url = url, headers = headers)
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    print(content)
except urllib.error.HTTPError:
    print('系统正在升级。。。')
except urllib.error.URLError:
    print('我都说了 系统正在升级。。。')
```
## 4.Cookie登录
数据采集的时候 需要绕过登陆 然后进入到某个页面
>个人信息页面是utf-8 但是还报错了编码错误
>
> 因为并没有进入到个人信息页面 而是跳转到了登陆页面
> 请求头的信息不够 所以访问不成功

```python
  
import urllib.request
url = 'https://weibo.cn/6451491586/info'

headers = {

# ':authority': 'weibo.cn',
# ':method': 'GET',
# ':path': '/6451491586/info',
# ':scheme': 'https',
# 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',

# 'accept-encoding': 'gzip, deflate, br',
# 'accept-language': 'zh-CN,zh;q=0.9',
# 'cache-control': 'max-age=0',

# cookie中携带着你的登陆信息 如果有登陆之后的cookie 那么我们就可以携带着cookie进入到任何页面
'cookie': '_T_WM=24c44910ba98d188fced94ba0da5960e; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFxxfgNNUmXi4YiaYZKr_J_5NHD95QcSh-pSh.pSKncWs4DqcjiqgSXIgvVPcpD; SUB=_2A25MKKG_DeRhGeBK7lMV-S_JwzqIHXVv0s_3rDV6PUJbktCOLXL2kW1NR6e0UHkCGcyvxTYyKB2OV9aloJJ7mUNz; SSOLoginState=1630327279',

# referer 判断当前路径是不是由上一个路径进来的 一般情况下 是做图片防盗链

'referer': 'https://weibo.cn/',
#'sec-ch-ua': '"Chromium";v="92", " Not A;Brand";v="99", "Google Chrome";v="92"',
#'sec-ch-ua-mobile': '?0',
#'sec-fetch-dest': 'document',
#'sec-fetch-mode': 'navigate',
#'sec-fetch-site': 'same-origin',
#'sec-fetch-user': '?1',
#'upgrade-insecure-requests': '1',
#'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
}

request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
# 将数据保存到本地
with open('weibo.html','w',encoding='utf-8')as fp:
    fp.write(content)
```

Handler

1. urllib.request.urlopen(url)：不能定制请求头
2. urllib.request.Request(url,headers,data)：可以定制请求头
3. Handler：定制更高级的请求头（随着业务逻辑的复杂 请求对象的定制已经满足不了我们的需求（动态cookie和代理不能使用请求对象的定制）
```python
import urllib.request
url = 'http://www.baidu.com'

headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:103.0) Gecko/20100101 Firefox/103.0'
}
  
request = urllib.request.Request(url = url,headers = headers)
  
# handler build_opener open

# （1）获取hanlder对象
handler = urllib.request.HTTPHandler()

# （2）获取opener对象
opener = urllib.request.build_opener(handler)

# (3) 调用open方法
response = opener.open(request)
content = response.read().decode('utf-8')
print(content)
```
## 5.代理服务器
1. 代理的常用功能?
  >突破自身IP访问限制，访问国外站点。
  >
  >访问一些单位或团体内部资源
  >
  > 提高访问速度
  > 
  > 隐藏真实IP
2. 代码配置代理
  >创建Reuqest对象
  >
  >创建ProxyHandler对象
  >
  >用handler对象创建opener对象
  >
  >使用opener.open函数发送请求
```python
# 请求对象的定制
request = urllib.request.Request(url=url, headers=headers)

# 模拟浏览器访问服务器
response = urllib.request.urlopen(request)

proxies = {
'http': '118.24.219.151:16817'
}

# handler build_opener open

handler = urllib.request.ProxyHandler(proxies=proxies)
opener = urllib.request.build_opener(handler)
response = opener.open(request)

# 获取响应的信息
content = response.read().decode('utf-8')

# 保存
with open('daili.html', 'w', encoding='utf-8')as fp:
    fp.write(content)
```

扩展：
1. 代理池
```python
proxies_pool = [
{'http':'118.24.219.151:16817'},
{'http':'118.24.219.151:16817'},
]

import random
proxies = random.choice(proxies_pool)
```
2. 快代理网站提供代理
