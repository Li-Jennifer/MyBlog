---
title: 解析
date: 2022-09-06 23:39:44
permalink: /pages/233bba/
categories:
  - 基础
  - 爬虫
tags:
  - 
author: 
  name: Jennifer
  link: https://github.com/Li-Jennifer
---

## 1. xpath
### 安装
安装xpath插件 
1. 打开chrome浏览器 
2. 点击右上角小圆点 
3. 更多工具 
4. 扩展程序 
5. 拖拽xpath插件到扩展程序中 
6. 如果crx文件失效，需要将后缀修改zip 
7. 再次拖拽 
8. 关闭浏览器重新打开 
9. ctrl + shift + x 
10. 出现小黑框

### 使用
1. 安装lxml库

`pip install lxml ‐i https://pypi.douban.com/simple`

2. xpath解析
- 本地文件 etree.parse
- 服务器响应的数据 response.read().decode('utf-8') ***** etree.HTML()
3. html_tree.xpath(xpath路径)


### xpath基本语法
```python
1. 路径查询
//：查找所有子孙节点，不考虑层级关系
/ ：找直接子节点

2. 谓词查询
//div[@id]
//div[@id="maincontent"]

3. 属性查询
//@class

4. 模糊查询
//div[contains(@id, "he")]
//div[starts‐with(@id, "he")]

5. 内容查询
//div/h1/text()

6. 逻辑运算
//div[@id="head" and @class="s_down"]
//title | //price
```
#### 例子
```python
from lxml import etree
tree = etree.parse('baidu.html')
tree.xpath('xpath路径')

# 查找ul下面的li
li_list = tree.xpath('//body/ul/li')

# 查找所有有id的属性的li标签
# text()获取标签中的内容
li_list = tree.xpath('//ul/li[@id]/text()')

# 找到id为l1的li标签 注意引号的问题
li_list = tree.xpath('//ul/li[@id="l1"]/text()')

# 查找到id为l1的li标签的class的属性值
li = tree.xpath('//ul/li[@id="l1"]/@class')

# 查询id中包含l的li标签
li_list = tree.xpath('//ul/li[contains(@id,"l")]/text()')

# 查询id的值以l开头的li标签
li_list = tree.xpath('//ul/li[starts-with(@id,"c")]/text()')

#查询id为l1和class为c1的
li_list = tree.xpath('//ul/li[@id="l1" and @class="c1"]/text()')
li_list = tree.xpath('//ul/li[@id="l1"]/text() | //ul/li[@id="l2"]/text()')

# 判断列表的长度
print(li_list)
print(len(li_list))
```
### 案例：下载的前十页的图片
```python
# （1） 获取网页的源码
# （2） 解析 解析的服务器响应的文件 etree.HTML
# (3) 打印

import urllib.request
from lxml import etree
def create_request(page):
    if(page == 1):
        url = 'https://sc.chinaz.com/tupian/qinglvtupian.html'
    else:
        url = 'https://sc.chinaz.com/tupian/qinglvtupian_' + str(page) + '.html'
  
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
    }

    request = urllib.request.Request(url = url, headers = headers)
    return request

def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content

def down_load(content):
    tree = etree.HTML(content)
    name_list = tree.xpath('//div[@id="container"]//a/img/@alt')
    src_list = tree.xpath('//div[@id="container"]//a/img/@src2')
    for i in range(len(name_list)):
        name = name_list[i]
        src = src_list[i]
        url = 'https:' + src
        urllib.request.urlretrieve(url=url,filename='./loveImg/' + name + '.jpg')

if __name__ == '__main__':
    start_page = int(input('请输入起始页码'))
    end_page = int(input('请输入结束页码'))
    for page in range(start_page,end_page+1):
    # (1) 请求对象的定制
    request = create_request(page)
    # （2）获取网页的源码
    content = get_content(request)
    # （3）下载
    down_load(content)
```

#### 参考链接
w3cshool: [https://www.w3school.com.cn/xpath/index.asp](https://www.w3school.com.cn/xpath/index.asp)
## 2.JsonPath
### 安装
`pip install jsonpath
```python
obj = json.load(open('json文件', 'r', encoding='utf‐8'))
ret = jsonpath.jsonpath(obj, 'jsonpath语法')
```

### 教程
[https://blog.csdn.net/luxideyao/article/details/77802389](https://blog.csdn.net/luxideyao/article/details/77802389)

### 案例:淘票票
```python
import urllib.request

url = 'https://dianying.taobao.com/cityAction.json?activityId&_ksTS=1629789477003_137&jsoncallback=jsonp138&action=cityAction&n_s=new&event_submit_doGetAllRegion=true'

headers = {
'accept': 'text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
'accept-language': 'zh-CN,zh;q=0.9',
'cookie': 'cna=UkO6F8VULRwCAXTqq7dbS5A8; miid=949542021157939863; sgcookie=E100F01JK9XMmyoZRigjfmZKExNdRHQqPf4v9NIWIC1nnpnxyNgROLshAf0gz7lGnkKvwCnu1umyfirMSAWtubqc4g%3D%3D; tracknick=action_li; _cc_=UIHiLt3xSw%3D%3D; enc=dA18hg7jG1xapfVGPHoQCAkPQ4as1%2FEUqsG4M6AcAjHFFUM54HWpBv4AAm0MbQgqO%2BiZ5qkUeLIxljrHkOW%2BtQ%3D%3D; hng=CN%7Czh-CN%7CCNY%7C156; thw=cn; _m_h5_tk=3ca69de1b9ad7dce614840fcd015dcdb_1629776735568; _m_h5_tk_enc=ab56df54999d1d2cac2f82753ae29f82; t=874e6ce33295bf6b95cfcfaff0af0db6; xlly_s=1; cookie2=13acd8f4dafac4f7bd2177d6710d60fe; v=0; _tb_token_=e65ebbe536158; tfstk=cGhRB7mNpnxkDmUx7YpDAMNM2gTGZbWLxUZN9U4ulewe025didli6j5AFPI8MEC..; l=eBrgmF1cOsMXqSxaBO5aFurza77tzIRb8sPzaNbMiInca6OdtFt_rNCK2Ns9SdtjgtfFBetPVKlOcRCEF3apbgiMW_N-1NKDSxJ6-; isg=BBoas2yXLzHdGp3pCh7XVmpja8A8S54lyLj1RySTHq14l7vRDNufNAjpZ2MLRxa9',
'referer': 'https://dianying.taobao.com/',
'sec-ch-ua': '"Chromium";v="92", " Not A;Brand";v="99", "Google Chrome";v="92"',
'sec-ch-ua-mobile': '?0',
'sec-fetch-dest': 'empty',
'sec-fetch-mode': 'cors',
'sec-fetch-site': 'same-origin',
'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
'x-requested-with': 'XMLHttpRequest',
}

request = urllib.request.Request(url = url, headers = headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')

# split 
content = content.split('(')[1].split(')')[0]
with open('解析淘票票.json','w',encoding='utf-8')as fp:
    fp.write(content)

  

import json
import jsonpath

obj = json.load(open('解析淘票票.json','r',encoding='utf-8'))
city_list = jsonpath.jsonpath(obj,'$..regionName')
print(city_list)
```

## 3.BeautifulSoup：bs4
### 安装使用
`pip install bs4`
```python
1. 服务器响应的文件生成对象
soup = BeautifulSoup(response.read().decode(), 'lxml')

2. 本地文件生成对象
soup = BeautifulSoup(open('1.html'), 'lxml')

注意：默认打开文件的编码格式gbk所以需要指定打开编码
```
### 节点定位
```python
1.根据标签名查找节点
soup.a # 只能找到第一个a
soup.a.name
soup.a.attrs

2.函数

(1).find(返回一个对象)
find('a')：只找到第一个a标签
find('a', title='名字')
find('a', class_='名字')

(2).find_all(返回一个列表)
find_all('a') 查找到所有的a
find_all(['a', 'span']) 返回所有的a和span
find_all('a', limit=2) 只找前两个a

(3).select(根据选择器得到节点对象)【推荐】

1.element
eg:p

2..class
eg:.firstname

3.#id
eg:#firstname

4.属性选择器 
[attribute]
eg:li = soup.select('li[class]')
[attribute=value]
eg:li = soup.select('li[class="hengheng1"]')

5.层级选择器
element element
div p
element>element
div>p
element,element
div,p
eg:soup = soup.select('a,span')
```
#### 节点信息
```python
(1).获取节点内容：适用于标签中嵌套标签的结构
obj.string
obj.get_text()【推荐】

(2).节点的属性
tag.name 获取标签名
eg:tag = find('li)
print(tag.name)
tag.attrs将属性值作为一个字典返回

(3).获取节点属性
obj.attrs.get('title')【常用】
obj.get('title')
obj['title']p
```
### 案例 ：星巴克
```python
import urllib.request
url = 'https://www.starbucks.com.cn/menu/'
response = urllib.request.urlopen(url)
content = response.read().decode('utf-8')

from bs4 import BeautifulSoup
soup = BeautifulSoup(content,'lxml')

# //ul[@class="grid padded-3 product"]//strong/text()
name_list = soup.select('ul[class="grid padded-3 product"] strong')

for name in name_list:
    print(name.get_text())
```